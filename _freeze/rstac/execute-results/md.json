{
  "hash": "c382e07b047648b2185365631fe4fb51",
  "result": {
    "markdown": "---\ntitle: \"Download data from a STAC API using R, rstac, and GDAL\"\nexecute:\n  freeze: auto\nformat:\n  html: default\n  md: default\n---\n\n\n\n\n\nThis tutorial walks through querying a STAC API using the [rstac](https://brazil-data-cube.github.io/rstac/) R package, and downloading data from the API using rstac or [GDAL](https://gdal.org/) (via [sf](https://github.com/r-spatial/sf)). This tutorial will assume that you're already familiar with [R](https://cran.r-project.org/) and working with spatial data.\n\nAs all of the packages we'll be using are available from CRAN, you can install them (if necessary) using `install.packages()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"sf\")\ninstall.packages(\"rstac\")\ninstall.packages(\"terra\")\n```\n:::\n\n\n\nSTAC APIs are servers that provide access to a set of data which users can query and retrieve. You can find a partial list of STAC APIs at [STAC Index, https://stacindex.org/](https://stacindex.org/). For this tutorial, we'll be downloading data from Microsoft's [Planetary Computer](https://planetarycomputer.microsoft.com), which currently provides more than 100 data sets for free via a central STAC API.\n\nTo start downloading data, we'll first need to let rstac know what STAC API we want to query and download from. To do so, we'll pass the URL of the Planetary Computer STAC API to the `rstac::stac()` function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstac_source <- rstac::stac(\n  \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n)\nstac_source\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n###RSTACQuery\n- url: https://planetarycomputer.microsoft.com/api/stac/v1\n- params:\n- field(s): version, base_url, endpoint, params, verb, encode\n```\n\n\n:::\n:::\n\n\n\nAs you can see, the output of `stac()` is an `RSTACQuery` object, which contains information about an HTTP query that we might want to run in the future. Under the hood, these objects are normal lists containing information about the query:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(stac_source)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 6\n $ version : NULL\n $ base_url: chr \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n $ endpoint: NULL\n $ params  : list()\n $ verb    : chr \"GET\"\n $ encode  : NULL\n - attr(*, \"class\")= chr [1:2] \"stac\" \"RSTACQuery\"\n```\n\n\n:::\n:::\n\n\n\nBut most of the time, you won't need to worry about this internal representation; rstac provides many helper functions to access the elements of this list if needed.\n\nIt's worth highlighting that this object is a representation of a *future* HTTP query, not the results of a query we've already run! In order to actually run these queries, we need to use `rstac::get_request()` (or `rstac::post_request()`, depending on what HTTP verb your STAC API is expecting). If we use `get_request()` to query the Planetary Computer STAC API, we get a brief description of what this API provides:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrstac::get_request(stac_source)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n###STACCatalog\n- id: microsoft-pc\n- description: \nSearchable spatiotemporal metadata describing Earth science datasets hosted by the Microsoft Planetary Computer\n- field(s): type, id, title, description, stac_version, conformsTo, links\n```\n\n\n:::\n:::\n\n\n\nBecause the `RSTACQuery` object is a representation of a future query, we can use other functions in rstac to change our query parameters and fields before we actually make a request. For instance, we can use `rstac::collections()` to update our request to query the `/collections` endpoint of the Planetary Computer API, which lists all the available [collections](https://github.com/radiantearth/stac-spec/blob/master/collection-spec/collection-spec.md) (which, to quote the STAC Collection specification, \"describe a group of [Items](https://github.com/radiantearth/stac-spec/blob/master/item-spec/item-spec.md) that share properties and metadata\"):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollections_query <- stac_source |>\n  rstac::collections()\n\ncollections_query\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n###RSTACQuery\n- url: https://planetarycomputer.microsoft.com/api/stac/v1\n- params:\n- field(s): version, base_url, endpoint, params, verb, encode\n```\n\n\n:::\n:::\n\n\n\nWhile it might not look like much has changed, under the hood our `collections_query` object has a new `collections` class to indicate that we're querying the collections endpoint, not the top-level STAC endpoint:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(stac_source)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"stac\"       \"RSTACQuery\"\n```\n\n\n:::\n\n```{.r .cell-code}\nclass(collections_query)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"collections\" \"RSTACQuery\" \n```\n\n\n:::\n:::\n\n\n\nAnd as a result, when we use `get_request()` to turn this query *specification* into a query *result*, we get a list of the collections available from this API:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navailable_collections <- rstac::get_request(collections_query)\navailable_collections\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n###STACCollectionList\n- collections (122 item(s)):\n  - daymet-annual-pr\n  - daymet-daily-hi\n  - 3dep-seamless\n  - 3dep-lidar-dsm\n  - fia\n  - sentinel-1-rtc\n  - gridmet\n  - daymet-annual-na\n  - daymet-monthly-na\n  - daymet-annual-hi\n  - ... with 112 more collection(s).\n- field(s): collections, links\n```\n\n\n:::\n:::\n\n\n\nThese collections are a subset of the data sets available in the [Planetary Computer data catalog](https://planetarycomputer.microsoft.com/catalog); the Planetary Computer is organized so that each collection corresponds to a distinct data set in the catalog.\n\nFor our purposes today, we're going to be querying the [USGS Land Change Monitoring, Assessment, and Projection (LCMAP)](https://planetarycomputer.microsoft.com/dataset/usgs-lcmap-conus-v13) collection, which provides (among other things) annual land cover classifications for the continental United States. We can query what items are available for this collection using the `rstac::stac_search()` function. We'll limit our search to 2021 using the `datetime` argument, and only search within the LCMAP collection by using the collection's ID of `usgs-lcmap-conus-v13`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrstac::stac_search(\n  q = stac_source,\n  collections = \"usgs-lcmap-conus-v13\",\n  datetime = \"2021-01-01/2021-12-31\"\n) |> \n  rstac::get_request()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n###STACItemCollection\n- features (250 item(s)):\n  - LCMAP_CU_032003_2021_V13_CCDC\n  - LCMAP_CU_031006_2021_V13_CCDC\n  - LCMAP_CU_031004_2021_V13_CCDC\n  - LCMAP_CU_031003_2021_V13_CCDC\n  - LCMAP_CU_031002_2021_V13_CCDC\n  - LCMAP_CU_030007_2021_V13_CCDC\n  - LCMAP_CU_030006_2021_V13_CCDC\n  - LCMAP_CU_030005_2021_V13_CCDC\n  - LCMAP_CU_030004_2021_V13_CCDC\n  - LCMAP_CU_030003_2021_V13_CCDC\n  - ... with 240 more feature(s).\n- assets: \nbrowse, dates, lcachg, lcachg_metadata, lcpconf, lcpconf_metadata, lcpri, lcpri_metadata, lcsconf, lcsconf_metadata, lcsec, lcsec_metadata, rendered_preview, sclast, sclast_metadata, scmag, scmag_metadata, scmqa, scmqa_metadata, scstab, scstab_metadata, sctime, sctime_metadata, tilejson\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n```\n\n\n:::\n:::\n\n\n\nWe can see that there are 250 items inside this catalog for 2021. Collectively, these items contain all LCMAP data for the continental United States for 2021, with each item containing a number of assets covering a relatively small chunk of the nation. These assets are \"object[s] that [contain] a URI to data associated with the Item that can be downloaded or streamed\", to quote [the spec](https://github.com/radiantearth/stac-spec/blob/master/item-spec/item-spec.md#asset-object); here, assets are things like \"primary land cover classification\" or \"a metadata object\".\n\nTo keep things simple, we'll start off downloading data for a relatively small region, namely North Carolina's Ashe County. We'll use data included in the sf package to get the county's geometry:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nashe <- sf::read_sf(system.file(\"shape/nc.shp\", package = \"sf\"))[1, ]\n\nsf::st_geometry(ashe) |> plot()\n```\n\n::: {.cell-output-display}\n![](rstac.markdown_strict_files/figure-markdown_strict/unnamed-chunk-10-1.png)\n:::\n:::\n\n\n\nTo filter our query down to just tiles intersecting this region, we'll need to provide the bounding box of the county in [WGS84](https://en.wikipedia.org/wiki/World_Geodetic_System) as a query parameter. We can use `sf::st_transform()` to reproject the county and `sf::st_bbox()` to find our bounding box:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nashe_bbox <- ashe |>\n  sf::st_transform(4326) |>\n  sf::st_bbox()\n\nashe_bbox\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     xmin      ymin      xmax      ymax \n-81.74091  36.23448 -81.23970  36.58977 \n```\n\n\n:::\n:::\n\n\n\nWe can then pass this bounding box directly to the `bbox` argument of `stac_search` to filter our results down further:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstac_query <- rstac::stac_search(\n  q = stac_source,\n  collections = \"usgs-lcmap-conus-v13\",\n  bbox = ashe_bbox,\n  datetime = \"2021-01-01/2021-12-31\"\n)\n\nexecuted_stac_query <- rstac::get_request(stac_query)\n\nexecuted_stac_query\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n###STACItemCollection\n- features (1 item(s)):\n  - LCMAP_CU_025011_2021_V13_CCDC\n- assets: \nbrowse, dates, lcachg, lcachg_metadata, lcpconf, lcpconf_metadata, lcpri, lcpri_metadata, lcsconf, lcsconf_metadata, lcsec, lcsec_metadata, rendered_preview, sclast, sclast_metadata, scmag, scmag_metadata, scmqa, scmqa_metadata, scstab, scstab_metadata, sctime, sctime_metadata, tilejson\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n```\n\n\n:::\n:::\n\n\n\nAs luck would have it, Ashe County is entirely contained in a single LCMAP tile. \n\nThere are a few different ways to download the assets associated with this item. First, we could use the `rstac::assets_download()` function. Before we can download assets, however, we're going to need to [authenticate ourselves with Planetary Computer](https://planetarycomputer.microsoft.com/docs/concepts/sas/). Note that this doesn't require an account or registration, but rather is a way for Microsoft to uniquely identify your Planetary Computer session and impose rate limits if needed.\n\nThis authentication process is built-in to rstac, using the `items_sign()` and `sign_planetary_computer()` functions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigned_stac_query <- rstac::items_sign(\n  executed_stac_query,\n  rstac::sign_planetary_computer()\n)\n\nsigned_stac_query\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n###STACItemCollection\n- features (1 item(s)):\n  - LCMAP_CU_025011_2021_V13_CCDC\n- assets: \nbrowse, dates, lcachg, lcachg_metadata, lcpconf, lcpconf_metadata, lcpri, lcpri_metadata, lcsconf, lcsconf_metadata, lcsec, lcsec_metadata, rendered_preview, sclast, sclast_metadata, scmag, scmag_metadata, scmqa, scmqa_metadata, scstab, scstab_metadata, sctime, sctime_metadata, tilejson\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n```\n\n\n:::\n:::\n\n\n\nWe didn't need to log in or register anywhere, but now have authenticated our requests and will be able to download assets from this STAC API.\n\nWe'll then use this signed query object with `assets_download()` to actually retrieve our assets. We'll provide the name of what assets we want to download -- let's start off by downloading `lcpri`, which contains the primary land cover classifications for this area:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrstac::assets_download(signed_stac_query, \"lcpri\", output_dir = tempdir())\n```\n:::\n\n\n\nWe're downloading these into a temporary directory (using `tempdir()`) so that these example files will be cleaned up after we finish running this tutorial. In practice, you'll likely want to set `output_dir` somewhere else, where the data will persist for as long as you need.\n\nThe `assets_download()` function will save your data in a sub-directory of `output_dir` that corresponds to the URL that the asset is being downloaded from. In this case, that winds up being quite a long folder path -- but once we type the folder path out, we can load our downloaded raster into R and use it with terra and other raster packages as normal:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutput_file <- file.path(\n  tempdir(),\n  \"lcmap\",\n  \"CU\",\n  \"V13\",\n  \"025011\",\n  \"2021\",\n  \"LCMAP_CU_025011_2021_20220721_V13_CCDC\",\n  \"LCMAP_CU_025011_2021_20220629_V13_LCPRI.tif\"\n) |>\n  terra::rast()\n\nterra::plot(output_file)\n```\n\n::: {.cell-output-display}\n![](rstac.markdown_strict_files/figure-markdown_strict/unnamed-chunk-15-1.png)\n:::\n:::\n\n\n\nThe `assets_download()` function is an easy and straightforward way to download assets associated with a STAC Item, and by passing multiple asset names (or leaving the `asset_names` argument `NULL`) we could use this function to download more than one asset at a time. This function can also handle any format an asset is stored in, making it a flexible way to download rasters, metadata, vectors, or whatever other data is provided by a STAC API.\n\nThe main downside of this approach, however, is that we can't use it to download only specific parts of an asset, which means we can wind up downloading much more data than we need. For instance, look at how far our land cover raster extends beyond Ashe County's borders:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nterra::plot(output_file)\nashe |>\n  sf::st_transform(sf::st_crs(output_file)) |>\n  sf::st_geometry() |>\n  plot(add = TRUE, lwd = 3)\n```\n\n::: {.cell-output-display}\n![](rstac.markdown_strict_files/figure-markdown_strict/unnamed-chunk-16-1.png)\n:::\n:::\n\n\n\nAn alternative approach for downloading raster assets is to use [GDAL's virtual file system interface](https://gdal.org/user/virtual_file_systems.html) to download rasters. If the STAC server stores the asset in a cloud-friendly format, such as [COG](https://www.cogeo.org/), then GDAL is able to download only the necessary pieces of the raster, potentially speeding up download times and reducing bandwidth usage.\n\nTo download assets using GDAL, we need to extract the URL for our assets from our _unsigned_ STAC query results. We can get the URL for the `lcpri` asset using `rstac::assets_url()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlcpri_url <- rstac::assets_url(executed_stac_query, \"lcpri\")\nlcpri_url\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"https://landcoverdata.blob.core.windows.net/lcmap/CU/V13/025011/2021/LCMAP_CU_025011_2021_20220721_V13_CCDC/LCMAP_CU_025011_2021_20220629_V13_LCPRI.tif\"\n```\n\n\n:::\n:::\n\n\n\nIn order for this URL to work with GDAL's virtual filesystem interface, we'll need to append a few things in front of the URL. Namely, we'll need to add:\n\n- `/vsicurl`, to specify that we want to use the [http-based virtual filesytem interface](https://gdal.org/user/virtual_file_systems.html#vsicurl-http-https-ftp-files-random-access),\n- `pc_url_signing=yes`, to use GDAL's built-in method for signing requests to Microsoft's Planetary Computer,\n- and `pc_collection=usgs-lcmap-conus-v13`, to let GDAL know what collection we need authentication for.\n\nAll of these configuration parameters are documented in the official [GDAL documentation](https://gdal.org/user/virtual_file_systems.html#vsicurl-http-https-ftp-files-random-access). \n\nWe can write a small function that will paste these parameters on to any URL we want:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_vsicurl_url <- function(base_url) {\n  paste0(\n    \"/vsicurl\", \n    \"?pc_url_signing=yes\",\n    \"&pc_collection=usgs-lcmap-conus-v13\",\n    \"&url=\",\n    base_url\n  )\n}\n```\n:::\n\n\n\nWe might have chosen to make this function a bit more flexible by adding a `collection` argument, so that we can sign requests for data in any collection we'd want. For our purposes here, however, this simpler function should suffice.\n\nWe can then use this function to modify our LCMAP URL:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlcpri_url <- make_vsicurl_url(lcpri_url)\n```\n:::\n\n\n\nNext, we'll download the `lcpri` asset from this URL using `sf::gdal_utils()`, which provides access to GDAL's C++ utilities. We'll use the included interface to [gdalwarp](https://gdal.org/programs/gdalwarp.html) to both download and reproject our data, using the `t_srs` and `te` arguments to gdalwarp to control the spatial reference system and the extent of our downloaded raster:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_file <- tempfile(fileext = \".tif\")\nsf::gdal_utils(\n  \"warp\",\n  source = lcpri_url,\n  destination = out_file,\n  options = c(\n    \"-t_srs\", sf::st_crs(ashe)$wkt,\n    \"-te\", sf::st_bbox(ashe)\n  )\n)\n```\n:::\n\n\n\nThis download runs a good bit faster than using `assets_download()`, and results in a smaller raster covering only our area of interest, reprojected into the same CRS as our original data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nterra::rast(out_file) |>\n  terra::plot()\nashe |>\n  sf::st_geometry() |>\n  plot(lwd = 3, add = TRUE)\n```\n\n::: {.cell-output-display}\n![](rstac.markdown_strict_files/figure-markdown_strict/unnamed-chunk-21-1.png)\n:::\n:::\n\n\n\nAt this point, we've walked through how to use rstac to query a STAC API, and how to use either rstac or GDAL to download assets found in these queries. It's worth highlighting that so far we've only needed to write about 40 lines of code; here's everything we've walked through so far, presented as a single script instead of in a bunch of chunks:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nashe <- sf::read_sf(system.file(\"shape/nc.shp\", package = \"sf\"))[1, ]\nashe_bbox <- ashe |>\n  sf::st_transform(4326) |>\n  sf::st_bbox()\n\nstac_query <- rstac::stac(\n  \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n) |>\n  rstac::stac_search(\n    collections = \"usgs-lcmap-conus-v13\",\n    bbox = ashe_bbox,\n    datetime = \"2021-01-01/2021-12-31\"\n  ) |>\n  rstac::get_request()\n\nmake_vsicurl_url <- function(base_url) {\n  paste0(\n    \"/vsicurl\", \n    \"?pc_url_signing=yes\",\n    \"&pc_collection=usgs-lcmap-conus-v13\",\n    \"&url=\",\n    base_url\n  )\n}\n\nlcpri_url <- make_vsicurl_url(rstac::assets_url(stac_query, \"lcpri\"))\n\nout_file <- tempfile(fileext = \".tif\")\nsf::gdal_utils(\n  \"warp\",\n  source = lcpri_url,\n  destination = out_file,\n  options = c(\n    \"-t_srs\", sf::st_crs(ashe)$wkt,\n    \"-te\", sf::st_bbox(ashe)\n  )\n)\n\nterra::rast(out_file) |>\n  terra::plot()\nashe |> \n  sf::st_geometry() |> \n  plot(lwd = 3, add = TRUE)\n```\n\n::: {.cell-output-display}\n![](rstac.markdown_strict_files/figure-markdown_strict/unnamed-chunk-22-1.png)\n:::\n:::\n\n\n\nWhat if we wanted to download more than one raster asset? We could choose to pass multiple asset names to `assets_download()`. For instance, if we wanted to also download the secondary land cover classification for this area (called `lcsec`), we could write:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrstac::assets_download(\n  signed_stac_query, \n  c(\"lcpri\", \"lcsec\"), \n  output_dir = tempdir(),\n  overwrite = TRUE\n)\n```\n:::\n\n\n\nNote that we needed to set `overwrite = TRUE`, in order to re-download the `lcpri` asset that we had already retrieved.\n\nThese rasters are now somewhere in our `output_dir` directory. We can find their file paths using `list.files()` in order to load and visualize them:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist.files(\n  file.path(tempdir(), \"lcmap\"),\n  recursive = TRUE,\n  full.names = TRUE\n) |> \n  terra::rast() |>\n  terra::plot()\n```\n\n::: {.cell-output-display}\n![](rstac.markdown_strict_files/figure-markdown_strict/unnamed-chunk-24-1.png)\n:::\n:::\n\n\n\nIf we wanted to download multiple assets using sf and GDAL, we'd need to run our `gdal_utils()` call for each asset we wanted to download. For instance, to download both `lcpri` and `lcsec`, we could run:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvapply(\n  make_vsicurl_url(rstac::assets_url(stac_query, c(\"lcpri\", \"lcsec\"))),\n  function(asset_url) {\n    out_file <- tempfile(fileext = \".tif\")\n    sf::gdal_utils(\n      \"warp\",\n      source = asset_url,\n      destination = out_file,\n      options = c(\n        \"-t_srs\", sf::st_crs(ashe)$wkt,\n        \"-te\", sf::st_bbox(ashe)\n      )\n    )\n    out_file\n  },\n  character(1)\n) |> \n  terra::rast() |> \n  terra::plot()\n```\n\n::: {.cell-output-display}\n![](rstac.markdown_strict_files/figure-markdown_strict/unnamed-chunk-25-1.png)\n:::\n:::\n\n\n\nWhat if we wanted to download the same asset from more than one item? For instance, what if we wanted to download `lcpri` for all of North Carolina?\n\nWe could construct our STAC query in almost exactly the same way as before, except using the entire `nc` data frame instead of just its first row:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnc <- sf::read_sf(system.file(\"shape/nc.shp\", package = \"sf\"))\nnc_bbox <- nc |>\n  sf::st_transform(4326) |>\n  sf::st_bbox()\n\nstac_query <- rstac::stac(\n  \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n) |>\n  rstac::stac_search(\n    collections = \"usgs-lcmap-conus-v13\",\n    bbox = nc_bbox,\n    datetime = \"2021-01-01/2021-12-31\"\n  ) |>\n  rstac::get_request()\n  \nsigned_query <- stac_query |>   \n  rstac::items_sign(\n    rstac::sign_planetary_computer()\n  )\n\nsigned_query\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n###STACItemCollection\n- features (20 item(s)):\n  - LCMAP_CU_029012_2021_V13_CCDC\n  - LCMAP_CU_029011_2021_V13_CCDC\n  - LCMAP_CU_028013_2021_V13_CCDC\n  - LCMAP_CU_028012_2021_V13_CCDC\n  - LCMAP_CU_028011_2021_V13_CCDC\n  - LCMAP_CU_028010_2021_V13_CCDC\n  - LCMAP_CU_027013_2021_V13_CCDC\n  - LCMAP_CU_027012_2021_V13_CCDC\n  - LCMAP_CU_027011_2021_V13_CCDC\n  - LCMAP_CU_027010_2021_V13_CCDC\n  - ... with 10 more feature(s).\n- assets: \nbrowse, dates, lcachg, lcachg_metadata, lcpconf, lcpconf_metadata, lcpri, lcpri_metadata, lcsconf, lcsconf_metadata, lcsec, lcsec_metadata, rendered_preview, sclast, sclast_metadata, scmag, scmag_metadata, scmqa, scmqa_metadata, scstab, scstab_metadata, sctime, sctime_metadata, tilejson\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n```\n\n\n:::\n:::\n\n\n\nAs you can see, we now have 20 items to download, rather than just 1!\n\nWe barely need to change our code at all to download all 20 `lcpri` assets from these items using `assets_download()`. We're simply going to set `progress = FALSE`, in order to keep the output from this function clean. Other than that, we don't need to change a thing:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrstac::assets_download(\n  signed_query, \n  \"lcpri\",\n  output_dir = tempdir(),\n  overwrite = TRUE,\n  progress = FALSE\n)\n```\n:::\n\n\n\nThese files are each saved into their own folder inside of `output_dir`. We can use `list.files()` to get the paths to each of them, and then `terra::sprc()` and `terra::mosaic()` to combine them into a single raster:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlcpri <- list.files(\n  file.path(tempdir(), \"lcmap\"),\n  pattern = \"LCPRI.tif\",\n  recursive = TRUE,\n  full.names = TRUE\n) |> \n  lapply(terra::rast) |> \n  terra::sprc() |> \n  terra::mosaic()\n\nterra::plot(lcpri)\nnc |> \n  sf::st_transform(sf::st_crs(lcpri)) |> \n  sf::st_geometry() |>\n  plot(lwd = 3, add = TRUE)\n```\n\n::: {.cell-output-display}\n![](rstac.markdown_strict_files/figure-markdown_strict/unnamed-chunk-28-1.png)\n:::\n:::\n\n\n\nNote that we've lost our color palette due to `mosaic()`, but this is the same data as we've been working with throughout the entire tutorial.\n\nOr alternatively, we can use our `gdal_utils()` call -- without any edits -- to download and merge these files together. Remember that we're using our _unsigned_ query results here!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_file <- tempfile(fileext = \".tif\")\nsf::gdal_utils(\n  \"warp\",\n  source = make_vsicurl_url(rstac::assets_url(stac_query, \"lcpri\")),\n  destination = out_file,\n  options = c(\n    \"-t_srs\", sf::st_crs(nc)$wkt,\n    \"-te\", sf::st_bbox(nc)\n  )\n)\n\nterra::rast(out_file) |>\n  terra::plot()\nnc |> \n  sf::st_geometry() |> \n  plot(lwd = 3, add = TRUE)\n```\n\n::: {.cell-output-display}\n![](rstac.markdown_strict_files/figure-markdown_strict/unnamed-chunk-29-1.png)\n:::\n:::\n\n\n\nNote that gdalwarp does not interpolate between overlapping pixels, and instead simply uses the value from whichever pixel it processed last. That's fine here, where our tiles shouldn't overlap (and pixel values should be identical if they do), but means you shouldn't use gdalwarp to combine images from multiple time periods into a single composite.\n\nIt's worth highlighting that these are just two methods among many for downloading this data -- you may also find using packages such as [terra](https://github.com/rspatial/terra) or [gdalcubes](https://gdalcubes.github.io/) useful for getting data from STAC APIs!\n",
    "supporting": [
      "rstac.markdown_strict_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}